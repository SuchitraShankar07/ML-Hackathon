{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyPzMypCYvBTrH0xPkD4bigs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SuchitraShankar07/ML-Hackathon/blob/master/ML_HACKATHON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy7yvjUhAcdv",
        "outputId": "738bb729-e23e-46bc-fe0e-5d2f1c06add8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Loaded 49972 cleaned words from corpus.txt\n",
            "Loaded 2000 test words from test.txt\n",
            "Training Enhanced HMM...\n",
            "Training Enhanced HMM models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training HMMs: 100%|██████████| 20/20 [00:00<00:00, 58.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained HMMs for 20 word lengths\n",
            "Learned 26 bigrams, 674 trigrams, 7782 4-grams\n",
            "Initializing enhanced agent...\n",
            "Training agent...\n",
            "Training hybrid agent for 40000 episodes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  13%|█▎        | 5006/40000 [01:39<11:15, 51.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 5000/40000, avg_reward= -10.76, eps_rl=0.0078, time=99.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  25%|██▌       | 10004/40000 [03:19<11:29, 43.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 10000/40000, avg_reward= -11.80, eps_rl=0.0020, time=199.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  38%|███▊      | 15007/40000 [05:58<11:47, 35.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 15000/40000, avg_reward= 10.86, eps_rl=0.0010, time=358.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  50%|█████     | 20002/40000 [08:32<18:50, 17.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 20000/40000, avg_reward= 10.79, eps_rl=0.0010, time=512.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  63%|██████▎   | 25005/40000 [11:11<07:02, 35.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 25000/40000, avg_reward= 55.19, eps_rl=0.0010, time=671.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  75%|███████▌  | 30006/40000 [13:51<04:56, 33.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 30000/40000, avg_reward= 56.36, eps_rl=0.0010, time=831.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  88%|████████▊ | 35003/40000 [16:28<02:10, 38.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 35000/40000, avg_reward= 54.73, eps_rl=0.0010, time=988.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 40000/40000 [19:05<00:00, 34.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 40000/40000, avg_reward= 53.99, eps_rl=0.0010, time=1145.8s\n",
            "Evaluating agent...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 2000/2000 [01:02<00:00, 32.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "                          EVALUATION SUMMARY                          \n",
            "======================================================================\n",
            "Games: 2000, Wins: 636, Success Rate: 31.80%\n",
            "Total wrong: 10468, Total repeated: 0\n",
            "Final Score: -51704.00\n",
            "======================================================================\n",
            "Total runtime: 1215.8s\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ENHANCED SCRIPT — Maximum Accuracy Hybrid HMM + Advanced Heuristics\n",
        "# ============================================================================\n",
        "# Part 1: Importing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict, Counter\n",
        "import dill as pickle\n",
        "import random\n",
        "from typing import List, Dict, Tuple, Set\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# ============================================================================\n",
        "# Part 2: Data Loading and Preprocessing\n",
        "# ============================================================================\n",
        "\n",
        "def load_corpus(filename='corpus.txt'):\n",
        "    \"\"\"Load and preprocess the word corpus (lowercase, a-z only).\"\"\"\n",
        "    with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        words = []\n",
        "        for line in f:\n",
        "            w = line.strip().lower()\n",
        "            if not w:\n",
        "                continue\n",
        "            w = re.sub(r'[^a-z]', '', w)\n",
        "            if 1 <= len(w) <= 20:\n",
        "                words.append(w)\n",
        "    print(f\"Loaded {len(words)} cleaned words from {filename}\")\n",
        "    return words\n",
        "\n",
        "\n",
        "def load_test_words(filename='test.txt'):\n",
        "    \"\"\"Load test words exactly as they appear (no preprocessing).\"\"\"\n",
        "    with open(filename, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        words = [line.strip() for line in f if line.strip()]\n",
        "    print(f\"Loaded {len(words)} test words from {filename}\")\n",
        "    return words\n",
        "\n",
        "# ============================================================================\n",
        "# Part 3: Enhanced HMM with Advanced Pattern Recognition\n",
        "# ============================================================================\n",
        "\n",
        "class HangmanHMM:\n",
        "    \"\"\"\n",
        "    Enhanced model with:\n",
        "      - Positional emission probabilities with adaptive smoothing\n",
        "      - Word frequency weighting\n",
        "      - Advanced n-gram modeling (bigrams, trigrams, 4-grams)\n",
        "      - Common letter pattern detection\n",
        "      - Double letter detection\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
        "        self.letter_to_idx = {c:i for i,c in enumerate(self.alphabet)}\n",
        "        self.idx_to_letter = {i:c for i,c in enumerate(self.alphabet)}\n",
        "        self.bigram_freq = defaultdict(Counter)\n",
        "        self.trigram_freq = defaultdict(Counter)\n",
        "        self.fourgram_freq = defaultdict(Counter)\n",
        "        self.overall_freq = Counter()\n",
        "        self.word_freq = Counter()\n",
        "        self.double_letter_freq = Counter()\n",
        "        self.common_endings = defaultdict(Counter)\n",
        "        self.common_beginnings = defaultdict(Counter)\n",
        "\n",
        "    def train(self, words: List[str]):\n",
        "        \"\"\"Train HMM models with enhanced pattern recognition.\"\"\"\n",
        "        print(\"Training Enhanced HMM models...\")\n",
        "\n",
        "        # Count word frequencies for weighting\n",
        "        for word in words:\n",
        "            self.word_freq[word] += 1\n",
        "\n",
        "        # Group words by length\n",
        "        words_by_length = defaultdict(list)\n",
        "        for word in words:\n",
        "            words_by_length[len(word)].append(word)\n",
        "\n",
        "            # Learn n-gram patterns with frequency weighting\n",
        "            freq_weight = np.log1p(self.word_freq[word])\n",
        "            for i, letter in enumerate(word):\n",
        "                self.overall_freq[letter] += freq_weight\n",
        "\n",
        "                # Detect double letters\n",
        "                if i > 0 and word[i-1] == letter:\n",
        "                    self.double_letter_freq[letter] += freq_weight\n",
        "\n",
        "                # N-grams\n",
        "                if i > 0:\n",
        "                    self.bigram_freq[word[i - 1]][letter] += freq_weight\n",
        "                if i > 1:\n",
        "                    bigram_key = word[i - 2 : i]\n",
        "                    self.trigram_freq[bigram_key][letter] += freq_weight\n",
        "                if i > 2:\n",
        "                    trigram_key = word[i - 3 : i]\n",
        "                    self.fourgram_freq[trigram_key][letter] += freq_weight\n",
        "\n",
        "                # Common patterns\n",
        "                if i < 3:\n",
        "                    self.common_beginnings[i][letter] += freq_weight\n",
        "                if i >= len(word) - 3:\n",
        "                    self.common_endings[len(word) - i - 1][letter] += freq_weight\n",
        "\n",
        "        # Train one emission model per word length\n",
        "        for length, word_list in tqdm(sorted(words_by_length.items()), desc=\"Training HMMs\"):\n",
        "            if len(word_list) < 3:\n",
        "                continue\n",
        "\n",
        "            # Emission probabilities with frequency weighting\n",
        "            emission_counts = np.zeros((length, 26))\n",
        "            start_letters = Counter()\n",
        "            end_letters = Counter()\n",
        "\n",
        "            for word in word_list:\n",
        "                freq_weight = np.log1p(self.word_freq[word])\n",
        "                start_letters[word[0]] += freq_weight\n",
        "                end_letters[word[-1]] += freq_weight\n",
        "\n",
        "                for pos, letter in enumerate(word):\n",
        "                    if letter in self.letter_to_idx:\n",
        "                        emission_counts[pos, self.letter_to_idx[letter]] += freq_weight\n",
        "\n",
        "            # Adaptive smoothing based on data size\n",
        "            smoothing = max(0.1, 1.0 / np.sqrt(len(word_list)))\n",
        "            emission_counts += smoothing\n",
        "            row_sums = emission_counts.sum(axis=1, keepdims=True)\n",
        "            row_sums[row_sums == 0] = 1.0\n",
        "            emission_probs = emission_counts / row_sums\n",
        "\n",
        "            # Store model\n",
        "            self.models[length] = {\n",
        "                \"emission_probs\": emission_probs,\n",
        "                \"word_list\": word_list,\n",
        "                \"start_letters\": start_letters,\n",
        "                \"end_letters\": end_letters,\n",
        "                \"word_set\": set(word_list),\n",
        "            }\n",
        "\n",
        "        print(f\"Trained HMMs for {len(self.models)} word lengths\")\n",
        "        print(f\"Learned {len(self.bigram_freq)} bigrams, {len(self.trigram_freq)} trigrams, {len(self.fourgram_freq)} 4-grams\")\n",
        "\n",
        "    def get_matching_words(self, masked_word: str, guessed_letters: Set[str]) -> List[str]:\n",
        "        L = len(masked_word)\n",
        "        if L not in self.models:\n",
        "            return []\n",
        "        word_list = self.models[L]['word_list']\n",
        "        pattern = '^' + re.escape(masked_word).replace('\\\\_', '.') + '$'\n",
        "        regex = re.compile(pattern)\n",
        "        matching = []\n",
        "        for w in word_list:\n",
        "            if not regex.match(w):\n",
        "                continue\n",
        "            ok = True\n",
        "            for i,(m,ch) in enumerate(zip(masked_word, w)):\n",
        "                if m == '_' and ch in guessed_letters:\n",
        "                    ok = False\n",
        "                    break\n",
        "                if m != '_' and m != ch:\n",
        "                    ok = False\n",
        "                    break\n",
        "            if ok:\n",
        "                matching.append(w)\n",
        "        return matching\n",
        "\n",
        "    def get_letter_probabilities(self, masked_word: str, guessed_letters: Set[str]) -> Dict[str,float]:\n",
        "        \"\"\"Enhanced probability calculation with multiple signals.\"\"\"\n",
        "        L = len(masked_word)\n",
        "        if L not in self.models:\n",
        "            total = sum(self.overall_freq.values()) or 1\n",
        "            probs = {c: self.overall_freq.get(c,0)/total for c in self.alphabet}\n",
        "            for g in guessed_letters:\n",
        "                probs[g] = 0.0\n",
        "            s = sum(probs.values()) or 1.0\n",
        "            return {c: probs[c]/s for c in probs}\n",
        "\n",
        "        model = self.models[L]\n",
        "        matching = self.get_matching_words(masked_word, guessed_letters)\n",
        "        counts = np.zeros(26)\n",
        "\n",
        "        if matching:\n",
        "            # Frequency-weighted candidate analysis\n",
        "            for w in matching:\n",
        "                freq_weight = np.log1p(self.word_freq.get(w, 1))\n",
        "                for i,ch in enumerate(w):\n",
        "                    if masked_word[i] == '_' and ch in self.letter_to_idx:\n",
        "                        counts[self.letter_to_idx[ch]] += freq_weight\n",
        "        else:\n",
        "            # Multi-signal positional approach\n",
        "            emission = model['emission_probs']\n",
        "\n",
        "            # Base positional probabilities\n",
        "            for i,ch in enumerate(masked_word):\n",
        "                if ch == '_' and i < emission.shape[0]:\n",
        "                    counts += emission[i] * 2.0  # Increased weight\n",
        "\n",
        "            # Advanced n-gram boosting\n",
        "            for i,ch in enumerate(masked_word):\n",
        "                if ch != '_':\n",
        "                    continue\n",
        "\n",
        "                # 4-gram boost (strongest signal)\n",
        "                if i > 2:\n",
        "                    if all(masked_word[j] != '_' for j in range(i-3, i)):\n",
        "                        trigram_key = masked_word[i-3:i]\n",
        "                        for letter, c in self.fourgram_freq.get(trigram_key, {}).items():\n",
        "                            if letter in self.letter_to_idx:\n",
        "                                counts[self.letter_to_idx[letter]] += c * 0.35\n",
        "\n",
        "                # Trigram boost\n",
        "                if i > 1:\n",
        "                    if masked_word[i-1] != '_' and masked_word[i-2] != '_':\n",
        "                        bg = masked_word[i-2:i]\n",
        "                        for letter, c in self.trigram_freq.get(bg, {}).items():\n",
        "                            if letter in self.letter_to_idx:\n",
        "                                counts[self.letter_to_idx[letter]] += c * 0.25\n",
        "\n",
        "                # Bigram boost (both directions)\n",
        "                if i > 0 and masked_word[i-1] != '_':\n",
        "                    prev = masked_word[i-1]\n",
        "                    for letter, c in self.bigram_freq.get(prev, {}).items():\n",
        "                        if letter in self.letter_to_idx:\n",
        "                            counts[self.letter_to_idx[letter]] += c * 0.15\n",
        "\n",
        "                if i < len(masked_word) - 1 and masked_word[i+1] != '_':\n",
        "                    next_char = masked_word[i+1]\n",
        "                    for letter, c in self.bigram_freq.items():\n",
        "                        if next_char in c and letter in self.letter_to_idx:\n",
        "                            counts[self.letter_to_idx[letter]] += c[next_char] * 0.12\n",
        "\n",
        "                # Double letter boost\n",
        "                if i > 0 and masked_word[i-1] != '_':\n",
        "                    prev = masked_word[i-1]\n",
        "                    if prev in self.letter_to_idx:\n",
        "                        counts[self.letter_to_idx[prev]] += self.double_letter_freq.get(prev, 0) * 0.2\n",
        "\n",
        "            # Position-specific patterns\n",
        "            for i, ch in enumerate(masked_word):\n",
        "                if ch == '_':\n",
        "                    if i < 3:\n",
        "                        for letter, c in self.common_beginnings.get(i, {}).items():\n",
        "                            if letter in self.letter_to_idx:\n",
        "                                counts[self.letter_to_idx[letter]] += c * 0.08\n",
        "                    if i >= len(masked_word) - 3:\n",
        "                        pos_from_end = len(masked_word) - i - 1\n",
        "                        for letter, c in self.common_endings.get(pos_from_end, {}).items():\n",
        "                            if letter in self.letter_to_idx:\n",
        "                                counts[self.letter_to_idx[letter]] += c * 0.08\n",
        "\n",
        "        # Zero out guessed letters\n",
        "        for g in guessed_letters:\n",
        "            if g in self.letter_to_idx:\n",
        "                counts[self.letter_to_idx[g]] = 0.0\n",
        "\n",
        "        s = counts.sum()\n",
        "        if s <= 0:\n",
        "            total = sum(self.overall_freq.values()) or 1\n",
        "            probs = {c: self.overall_freq.get(c,0)/total for c in self.alphabet}\n",
        "            for g in guessed_letters:\n",
        "                probs[g] = 0.0\n",
        "            ss = sum(probs.values()) or 1.0\n",
        "            return {c: probs[c]/ss for c in probs}\n",
        "\n",
        "        probs = counts / s\n",
        "        return {self.idx_to_letter[i]: float(probs[i]) for i in range(26)}\n",
        "\n",
        "# ============================================================================\n",
        "# Part 4: Hangman Game Environment\n",
        "# ============================================================================\n",
        "\n",
        "class HangmanEnvironment:\n",
        "    def __init__(self, word: str, max_lives: int = 6):\n",
        "        self.word = word.lower()\n",
        "        self.max_lives = max_lives\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.lives = self.max_lives\n",
        "        self.guessed_letters = set()\n",
        "        self.masked_word = '_' * len(self.word)\n",
        "        self.wrong_guesses = 0\n",
        "        self.repeated_guesses = 0\n",
        "        self.done = False\n",
        "        self.won = False\n",
        "        self.num_revealed = 0\n",
        "        return self.get_state()\n",
        "\n",
        "    def get_state(self) -> Dict:\n",
        "        return {\n",
        "            'masked_word': self.masked_word,\n",
        "            'guessed_letters': set(self.guessed_letters),\n",
        "            'lives': self.lives,\n",
        "            'wrong_guesses': self.wrong_guesses,\n",
        "            'repeated_guesses': self.repeated_guesses,\n",
        "            'done': self.done,\n",
        "            'won': self.won,\n",
        "            'num_revealed': self.num_revealed,\n",
        "            'progress': (self.num_revealed / len(self.word)) if len(self.word)>0 else 0.0\n",
        "        }\n",
        "\n",
        "    def step(self, letter: str) -> Tuple[Dict, float, bool]:\n",
        "        letter = letter.lower()\n",
        "        if letter in self.guessed_letters:\n",
        "            self.repeated_guesses += 1\n",
        "            self.lives -= 1\n",
        "            if self.lives <= 0:\n",
        "                self.done = True\n",
        "                self.won = False\n",
        "            return self.get_state(), -2.0, self.done\n",
        "\n",
        "        self.guessed_letters.add(letter)\n",
        "        if letter in self.word:\n",
        "            new_mask = list(self.masked_word)\n",
        "            revealed = 0\n",
        "            for i,ch in enumerate(self.word):\n",
        "                if ch == letter and new_mask[i] == '_':\n",
        "                    new_mask[i] = letter\n",
        "                    revealed += 1\n",
        "            self.masked_word = ''.join(new_mask)\n",
        "            self.num_revealed += revealed\n",
        "            reward = 5.0 + (8.0 * revealed)\n",
        "            if '_' not in self.masked_word:\n",
        "                self.done = True\n",
        "                self.won = True\n",
        "                reward += 80.0 + (10.0 * self.lives)\n",
        "        else:\n",
        "            self.lives -= 1\n",
        "            self.wrong_guesses += 1\n",
        "            reward = -8.0\n",
        "            if self.lives <= 0:\n",
        "                self.done = True\n",
        "                self.won = False\n",
        "                reward -= 40.0\n",
        "        return self.get_state(), reward, self.done\n",
        "\n",
        "# ============================================================================\n",
        "# Part 5: Enhanced Hybrid Agent with Smart Heuristics\n",
        "# ============================================================================\n",
        "\n",
        "class HybridHangmanAgent:\n",
        "    \"\"\"\n",
        "    Enhanced decision strategy:\n",
        "     - Smarter candidate filtering\n",
        "     - Frequency-weighted selection\n",
        "     - Context-aware disambiguation\n",
        "     - Minimal RL for edge cases\n",
        "    \"\"\"\n",
        "    def __init__(self, hmm_model: HangmanHMM,\n",
        "                 epsilon_rl: float = 0.02,\n",
        "                 epsilon_decay: float = 0.99997,\n",
        "                 epsilon_min: float = 0.001,\n",
        "                 lr: float = 0.1, gamma: float = 0.96):\n",
        "        self.hmm = hmm_model\n",
        "        self.epsilon_rl = epsilon_rl\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
        "        self.q_table = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    def state_key(self, state: Dict) -> str:\n",
        "        mk = state['masked_word']\n",
        "        guessed = ''.join(sorted(state['guessed_letters']))\n",
        "        lives = state['lives']\n",
        "        prog = int(state['progress']*10)\n",
        "        return f\"{mk}|{guessed}|{lives}|{prog}\"\n",
        "\n",
        "    def choose_action(self, state: Dict, training: bool = True) -> str:\n",
        "        available = [c for c in self.alphabet if c not in state['guessed_letters']]\n",
        "        if not available:\n",
        "            return random.choice(list(self.alphabet))\n",
        "\n",
        "        # Very minimal RL exploration\n",
        "        if training and random.random() < self.epsilon_rl:\n",
        "            return random.choice(available)\n",
        "\n",
        "        masked = state['masked_word']\n",
        "        guessed = set(state['guessed_letters'])\n",
        "        matching = self.hmm.get_matching_words(masked, guessed)\n",
        "\n",
        "        # Single candidate: deterministic play\n",
        "        if len(matching) == 1:\n",
        "            cand = matching[0]\n",
        "            for ch in cand:\n",
        "                if ch not in guessed:\n",
        "                    return ch\n",
        "\n",
        "        # Small candidate set: frequency-weighted smart selection\n",
        "        if 1 < len(matching) <= 15:\n",
        "            freq = Counter()\n",
        "            for w in matching:\n",
        "                word_weight = np.log1p(self.hmm.word_freq.get(w, 1))\n",
        "                for i, ch in enumerate(w):\n",
        "                    if masked[i] == '_' and ch not in guessed:\n",
        "                        freq[ch] += word_weight\n",
        "\n",
        "            if freq:\n",
        "                # Pick letter that disambiguates most effectively\n",
        "                best = freq.most_common(1)[0][0]\n",
        "                return best\n",
        "\n",
        "        # Fallback: use enhanced HMM probabilities\n",
        "        probs = self.hmm.get_letter_probabilities(masked, guessed)\n",
        "        best_letter = max(available, key=lambda c: probs.get(c, 0.0))\n",
        "        return best_letter\n",
        "\n",
        "    def update(self, state: Dict, action: str, reward: float, next_state: Dict, done: bool):\n",
        "        s = self.state_key(state)\n",
        "        ns = self.state_key(next_state)\n",
        "        cur = self.q_table[s].get(action, 0.0)\n",
        "        if done:\n",
        "            nxt_max = 0.0\n",
        "        else:\n",
        "            av = [a for a in self.alphabet if a not in next_state['guessed_letters']]\n",
        "            nxt_max = max([self.q_table[ns].get(a,0.0) for a in av], default=0.0)\n",
        "        td = reward + self.gamma * nxt_max - cur\n",
        "        self.q_table[s][action] = cur + self.lr * td\n",
        "        if self.epsilon_rl > self.epsilon_min:\n",
        "            self.epsilon_rl *= self.epsilon_decay\n",
        "\n",
        "# ============================================================================\n",
        "# Part 6: Training Pipeline\n",
        "# ============================================================================\n",
        "\n",
        "def train_agent(agent: HybridHangmanAgent, train_words: List[str], num_episodes: int = 25000):\n",
        "    print(f\"Training hybrid agent for {num_episodes} episodes\")\n",
        "    sorted_words = sorted(train_words, key=len)\n",
        "    curriculum_size = max(1, len(sorted_words)//4)\n",
        "    episode_rewards = []\n",
        "    start = time.time()\n",
        "\n",
        "    for ep in tqdm(range(num_episodes), desc=\"Training\"):\n",
        "        if ep < num_episodes//4:\n",
        "            word = random.choice(sorted_words[:curriculum_size])\n",
        "        elif ep < num_episodes//2:\n",
        "            word = random.choice(sorted_words[:min(len(sorted_words), curriculum_size*2)])\n",
        "        else:\n",
        "            word = random.choice(train_words)\n",
        "\n",
        "        env = HangmanEnvironment(word)\n",
        "        state = env.reset()\n",
        "        total_reward = 0.0\n",
        "        steps = 0\n",
        "\n",
        "        while not state['done']:\n",
        "            action = agent.choose_action(state, training=True)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            agent.update(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            steps += 1\n",
        "            if steps > 26:\n",
        "                break\n",
        "\n",
        "        episode_rewards.append(total_reward)\n",
        "\n",
        "        if (ep+1) % 5000 == 0:\n",
        "            elapsed = time.time() - start\n",
        "            avg_recent = np.mean(episode_rewards[-5000:])\n",
        "            print(f\"Ep {ep+1}/{num_episodes}, avg_reward= {avg_recent:.2f}, eps_rl={agent.epsilon_rl:.4f}, time={elapsed:.1f}s\")\n",
        "\n",
        "    return episode_rewards\n",
        "\n",
        "# ============================================================================\n",
        "# Part 7: Evaluation Pipeline\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_agent(agent: HybridHangmanAgent, test_words: List[str], num_games: int = 2000, verbose_sample: int = 100):\n",
        "    wins = 0\n",
        "    total_wrong = 0\n",
        "    total_repeated = 0\n",
        "    game_details = []\n",
        "    test_sample = [test_words[i % len(test_words)] for i in range(num_games)]\n",
        "\n",
        "    for i, w in enumerate(tqdm(test_sample, desc=\"Evaluating\")):\n",
        "        env = HangmanEnvironment(w)\n",
        "        state = env.reset()\n",
        "        guesses = []\n",
        "        per_step_log = []\n",
        "\n",
        "        while not state['done']:\n",
        "            action = agent.choose_action(state, training=False)\n",
        "            guesses.append(action)\n",
        "            next_state, _, _ = env.step(action)\n",
        "            if i < verbose_sample:\n",
        "                per_step_log.append({\n",
        "                    'guess': action,\n",
        "                    'masked': next_state['masked_word'],\n",
        "                    'lives': next_state['lives']\n",
        "                })\n",
        "            state = next_state\n",
        "\n",
        "        if state['won']:\n",
        "            wins += 1\n",
        "        total_wrong += state['wrong_guesses']\n",
        "        total_repeated += state['repeated_guesses']\n",
        "\n",
        "        if i < verbose_sample:\n",
        "            game_details.append({\n",
        "                'word': w,\n",
        "                'won': state['won'],\n",
        "                'wrong': state['wrong_guesses'],\n",
        "                'repeated': state['repeated_guesses'],\n",
        "                'guesses': guesses,\n",
        "                'log': per_step_log\n",
        "            })\n",
        "\n",
        "    success_rate = wins / num_games\n",
        "    final_score = (success_rate * num_games) - (total_wrong * 5) - (total_repeated * 2)\n",
        "\n",
        "    results = {\n",
        "        'num_games': num_games,\n",
        "        'wins': wins,\n",
        "        'success_rate': success_rate,\n",
        "        'total_wrong_guesses': total_wrong,\n",
        "        'total_repeated_guesses': total_repeated,\n",
        "        'final_score': final_score,\n",
        "        'sample_games': game_details\n",
        "    }\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"EVALUATION SUMMARY\".center(70))\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Games: {num_games}, Wins: {wins}, Success Rate: {success_rate:.2%}\")\n",
        "    print(f\"Total wrong: {total_wrong}, Total repeated: {total_repeated}\")\n",
        "    print(f\"Final Score: {final_score:.2f}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================================================\n",
        "# Part 8: Main Execution\n",
        "# ============================================================================\n",
        "\n",
        "def main(num_train_episodes=25000, eval_games=2000):\n",
        "    t0 = time.time()\n",
        "    print(\"Loading data...\")\n",
        "    train_words = load_corpus('corpus.txt')\n",
        "    test_words = load_test_words('test.txt')\n",
        "\n",
        "    print(\"Training Enhanced HMM...\")\n",
        "    hmm = HangmanHMM()\n",
        "    hmm.train(train_words)\n",
        "\n",
        "    print(\"Initializing enhanced agent...\")\n",
        "    agent = HybridHangmanAgent(hmm_model=hmm,\n",
        "                               epsilon_rl=0.03,\n",
        "                               epsilon_decay=0.99997,\n",
        "                               epsilon_min=0.001,\n",
        "                               lr=0.1, gamma=0.96)\n",
        "\n",
        "    print(\"Training agent...\")\n",
        "    episode_rewards = train_agent(agent, train_words, num_episodes=num_train_episodes)\n",
        "\n",
        "    print(\"Evaluating agent...\")\n",
        "    results = evaluate_agent(agent, test_words, num_games=eval_games, verbose_sample=100)\n",
        "\n",
        "    with open('trained_hybrid_agent.pkl', 'wb') as f:\n",
        "        pickle.dump({'agent': agent, 'hmm': hmm}, f)\n",
        "    with open('eval_results.json', 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    t1 = time.time()\n",
        "    print(f\"Total runtime: {t1 - t0:.1f}s\")\n",
        "    return agent, hmm, results, episode_rewards\n",
        "\n",
        "# ============================================================================\n",
        "# Part 9: Run Everything\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    agent, hmm, results, episode_rewards = main(num_train_episodes=40000, eval_games=2000)"
      ]
    }
  ]
}